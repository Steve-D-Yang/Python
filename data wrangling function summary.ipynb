{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\06050\\anaconda3\\envs\\minimal_ds\\lib\\site-packages (0.12.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement as (from versions: none)\n",
      "ERROR: No matching distribution found for as\n"
     ]
    }
   ],
   "source": [
    "#install packages\n",
    "! pip install seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "#uninstall packages\n",
    "! pip uninstall seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " #import all the functions from a module (pandas: data wrangling package)\n",
    " import pandas as pd\n",
    " import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pear\n"
     ]
    }
   ],
   "source": [
    "#import parts of the functions from a module\n",
    "\n",
    "from random import choice\n",
    "fruit = [\"Apple\",\"Pear\",\"Banana\"]\n",
    "print(choice(fruit))\n",
    "\n",
    "from requests import Request, Session\n",
    "from requests.exceptions import ConnectionError, Timeout, TooManyRedirects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading and writing files(CSV,EXCEL,SQL and much more)\n",
    "# 1. getting the path of the working directory \n",
    "import os #provides functions for interacting with the operating system\n",
    "pwd = os.getcwd() \n",
    "# 2. creating a string that is the filepath to the file\n",
    "filepath = pwd + \"\\\\API.csv\"\n",
    "# 3.reading csv file \n",
    "df = pd.read_csv(filepath)\n",
    "# 4.reading excel file\n",
    "smo = pd.read_excel(pwd + \"\\\\\"+\"Data - Survey Monkey Output Edited.xlsx\", sheet_name=\"Edited_Data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.reading multiple csv files\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# FILE PATHS WITH GLOB ----\n",
    "path = \"06_read_many_csv/car_data/\"\n",
    "all_files = glob.glob(path + \"*.csv\")\n",
    "\n",
    "all_files\n",
    "\n",
    "# METHOD 1: \n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "li[] \n",
    "\n",
    "df_method1 = pd.concat(li, axis=0, ignore_index=True) # axis=0 row wise binding\n",
    "df_method1\n",
    "\n",
    "# METHOD 2: MAP ----\n",
    "\n",
    "\n",
    "li_mapper = map(lambda filename: pd.read_csv(filename, index_col=None, header=0), all_files)\n",
    "\n",
    "li_2 = list(li_mapper)\n",
    "\n",
    "df_method2 = pd.concat(li_2, axis=0, ignore_index=True)\n",
    "df_method2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. writing excel file\n",
    "df.to_excel(pwd + \"/Final_Output.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Merge data from each month into one CSV\n",
    "\n",
    "path = \"./Sales_Data\"\n",
    "files = [file for file in os.listdir(path) if not file.startswith('.')] # Ignore hidden files = glob\n",
    "\n",
    "all_months_data = pd.DataFrame()\n",
    "\n",
    "for file in files:\n",
    "    current_data = pd.read_csv(path+\"\\\\\"+file)\n",
    "    all_months_data = pd.concat([all_months_data, current_data])\n",
    "    \n",
    "all_months_data.to_csv(\"all_data_copy.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.Changing my CWD\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. writing csv file\n",
    "final_concat_data.to_csv('CoronaVirus PowerBI Raw - Cumulative Test', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. return only the file names\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data wrangling (basic)\n",
    "#Data cleaningï¼šAs I perform operations and get errors. Based on the error, figuring out how to clean the data\n",
    "\n",
    "#1. Columns \n",
    "df[\"Column1\"]\n",
    "df[\"New_Column2\"] = range(100)\n",
    "df[\"Adding Stuff\"] = df[\"Column3\"] + df[\"Column1\"]\n",
    "df_drop = df.drop(columns=\"Column2\")\n",
    "df.drop(columns=\"Columns3\", inplace =True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data wrangling (basic)\n",
    "#2.row loc and iloc\n",
    "df.loc[:, \"Column1\"]\n",
    "df.loc[0:4, \"Column1\"] # Select rows 0 through 4 INCLUSIVE (different from other Python indices) in Column1\n",
    "df.loc[:, [\"Column1\", \"New_Column2\"]] \n",
    "full_join3['Confirmed Daily'].loc[full_join3['Date'] == '2020-01-22'] = full_join3['Confirmed']\n",
    "\n",
    "\n",
    "df.iloc[:, 1] \n",
    "df.iloc[0:4, 1] # Select rows 0 through 4 excluding 4 (different from other Python indices) in Column1\n",
    "df.iloc[:, 0:2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data wrangling (basic)\n",
    "# 3.list out columns\n",
    "df.columns\n",
    "\n",
    "# 4.Basic Descriptive Statistics\n",
    "df.describe()\n",
    "\n",
    "# 5.See Unique Values in a Column\n",
    "df[\"type\"].unique()\n",
    "\n",
    "#Filtering DataFrames\n",
    "df[df[\"country\"] == \"Singapore\"] # single condition\n",
    "df[(df[\"country\"] == \"Singapore\") & (df[\"rating\"] == \"TV-MA\")]#multiple condition (It is important to enclose each case in parentheses)\n",
    "df10 = df[['name','quote.USD.price','timestamp']]\n",
    "df10 = df10.query(\"name == 'Bitcoin'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data wrangling (basic)\n",
    "# 6.datetimes:change different formats of dates into the same\n",
    "df[\"computer_date\"] = pd.to_datetime(df[\"date_added\"])\n",
    "\n",
    "# 7.datetimes:fill in the NAT values for dates\n",
    "import datetime\n",
    "df[\"computer_date\"].fillna(datetime.datetime(2020, 1,1), inplace=True)\n",
    "\n",
    "# 8.datetimes:Unix Time\n",
    "df[\"unix_time\"] = df[\"computer_date\"].astype(int)\n",
    "\n",
    "# 9.adding timestamp\n",
    "df['timestamp'] = pd.to_datetime('now')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data wrangling (medium and frequently used)\n",
    "# 10.strip\n",
    "price = df.strip()[1:]\n",
    "\n",
    "# 11.normalizes\n",
    "df = pd.json_normalize(data['data'])\n",
    "\n",
    "# 12.spliting columns\n",
    "df[[\"Date Part 1\", \"Date Part 2\"]] = df[\"date_added\"].str.split(\", \", expand=True)\n",
    "\n",
    "# 13.renaming columns\n",
    "df.rename(columns={\"Date Part 2\": \"Year\"}, inplace=True)\n",
    "\n",
    "df.rename(columns={\"Identify which division you work in. - Response\":\"Division Primary\", \"Identify which division you work in. - Other (please specify)\":\"Division Secondary\", \"Which of the following best describes your position level? - Response\":\"Position\", \"Which generation are you apart of? - Response\":\"Generation\", \"Please select the gender in which you identify. - Response\":\"Gender\"}, inplace=True)\n",
    "\n",
    "\n",
    "# 14.Fill Na\n",
    "df[\"cast\"].fillna(value=\"no cast\", inplace=True)\n",
    "\n",
    "# 15. map function\n",
    "df[\"genre_count\"] = df[\"listed_in\"].map(lambda x: len(x.split(\",\")) \n",
    "\n",
    "# 16. apply\n",
    "df[\"nonsensical_columns\"] = df.apply(lambda x: len(x[\"cast\"].split(\",\")) + len(x[\"listed_in\"].split(\",\")), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data wrangling (medium and frequently used)\n",
    "# 17.group by, aggregation and sorting\n",
    "netflix_movies_by_country = df.groupby(\"country\")[\"show_id\"].count().reset_index().sort_values(by=\"show_id\", ascending=False)\n",
    "\n",
    "df = df.groupby('name', sort=False)[['quote.USD.percent_change_1h','quote.USD.percent_change_24h','quote.USD.percent_change_7d','quote.USD.percent_change_30d','quote.USD.percent_change_60d','quote.USD.percent_change_90d']].mean()\n",
    "\n",
    "same_answer = df.groupby([\"Question + Subquestion\", \"Answer\"])[\"Respondent ID\"].nunique().reset_index()\n",
    "\n",
    "df.groupby(['company', 'year'])[[\"gross\"]].sum()\n",
    "\n",
    "#group in order\n",
    "keys = [city for city, df in all_data.groupby(['City'])]\n",
    "plt.bar(keys,all_data.groupby(['City']).sum()['Sales'])\n",
    "\n",
    "# 18.Unions\n",
    "new_dataset = pd.concat([df, second_dataset])\n",
    "\n",
    "# 19.merge\n",
    "netflix_merged = pd.merge(left = new_dataset, right = netflix_movies_by_country, how =\"inner\", left_on = [\"country\"], right_on = [\"country\"])\n",
    "\n",
    "# 20.Pivot\n",
    "pivot_table = netflix.pivot_table(index=\"country\", columns=\"type\", values= \"title\", aggfunc='count', fill_value=0).reset_index()\n",
    "\n",
    "# 21. Melt (unpivot)\n",
    "# id_var:which columns stay the same, value_vars: which values will be unpivoted\n",
    "\n",
    "id_vars = list(dataset_modified.columns)[ : 8]\n",
    "value_vars = list(dataset_modified.columns)[8 : ]\n",
    "dataset_melted = dataset_modified.melt(id_vars=id_vars, value_vars = value_vars, var_name=\"Question + Subquestion\", value_name=\"Answer\")\n",
    "\n",
    "df = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'},\n",
    "                   'B': {0: 1, 1: 3, 2: 5},\n",
    "                   'C': {0: 2, 1: 4, 2: 6}})\n",
    "\n",
    "df.melt(id_vars=['A'], value_vars=['B'],\n",
    "        var_name='myVarname', value_name='myValname')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22.stack\n",
    "df = df.stack()\n",
    "\n",
    "# 23.unstack\n",
    "correlation_mat = df.apply(lambda x: x.factorize()[0]).corr()\n",
    "corr_pairs = correlation_mat.unstack()\n",
    "print(corr_pairs)\n",
    "\n",
    "# 24.count\n",
    "df.count()\n",
    "\n",
    "# 25.reset_index\n",
    "df = df.reset_index()\n",
    "\n",
    "# 26.replace\n",
    "df['percent_change'] = df['percent_change'].replace(['quote.USD.percent_change_30d','quote.USD.percent_change_60d','quote.USD.percent_change_90d'],['30d','60d','90d'])\n",
    "\n",
    "# 27.copy\n",
    "dataset_modified = dataset.copy()\n",
    "\n",
    "# 28.drop\n",
    "columns_to_drop = ['Start Date', 'End Date', 'Email Address', 'First Name', 'Last Name', 'Custom Data 1']\n",
    "dataset_modified = dataset_modified.drop(columns=columns_to_drop)\n",
    "\n",
    "questions.drop(columns=[\"Raw Question\", \"Raw Subquestion\", \"Subquestion\"], inplace=True)\n",
    "\n",
    "# 29.dropna\n",
    "questions.dropna(inplace=True)\n",
    "\n",
    "# 30.notna\n",
    "respondents = dataset[dataset[\"Answer\"].notna()]\n",
    "\n",
    "# 31.isnul\n",
    "for col in df.columns:\n",
    "    pct_missing = np.mean(df[col].isnull())\n",
    "    print('{} - {}%'.format(col, round(pct_missing*100)))\n",
    "\n",
    "# 32.dtype & astype\n",
    "print(df.dtypes)\n",
    "\n",
    "for col_name in df.columns:\n",
    "    if(df[col_name].dtype == 'object'):\n",
    "        df[col_name]= df[col_name].astype('category')\n",
    "        df[col_name] = df[col_name].cat.codes\n",
    "\n",
    "# 33.drop duolicates\n",
    "df.drop_duplicates()\n",
    "\n",
    "# 34.sorting\n",
    "df.sort_values(by=['gross'], inplace=True, ascending=False)\n",
    "CompanyGrossSumSorted = CompanyGrossSum.sort_values('gross', ascending = False)[:15]\n",
    "\n",
    "\n",
    "# 35.absolute value\n",
    "strong_pairs = sorted_pairs[abs(sorted_pairs) > 0.5]\n",
    "\n",
    "\n",
    "# 36.cat.codes: randomly set category into number\n",
    "df[cat_columns] = df[cat_columns].apply(lambda x: x.cat.codes)\n",
    "\n",
    "\n",
    "# 37.df\n",
    "  all_data['Month 2'] = pd.to_datetime(all_data['Order Date']).dt.month\n",
    "  all_data['Hour'] = pd.to_datetime(all_data['Order Date']).dt.hour\n",
    "  all_data['Minute'] = pd.to_datetime(all_data['Order Date']).dt.minute\n",
    "\n",
    "# 38.str\n",
    "  all_data['Month'] = all_data['Order Date'].str[0:2]\n",
    "  all_data['Month'] = all_data['Month'].astype('int32')\n",
    "\n",
    "\n",
    "# 39.duplicated\n",
    "df = all_data[all_data['Order ID'].duplicated(keep=False)] #keep everything\n",
    "\n",
    "# 40.shape\n",
    "print(\"The Shape of Cornirmed is: \", raw_data_confirmed2.shape)\n",
    "\n",
    "# 41.timedelta\n",
    "full_join2['Date - 1'] = full_join2['Date'] + pd.Timedelta(days=1)\n",
    "\n",
    "# 42.join()\n",
    "df2=pd.read_csv('Sales_December_2019.csv')\n",
    "df2= df2.dropna()\n",
    "df2 =df2[df2['Order ID'].duplicated(keep=False)]\n",
    "df2['Group'] = df2.groupby('Order ID')['Product'].transform(lambda x:','.join(x))\n",
    "df2 =df2[['Order ID', 'Group']].drop_duplicates()\n",
    "df2.head()\n",
    "\n",
    "# 43.counter & combinations & most_common\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "\n",
    "count = Counter()\n",
    "\n",
    "for row in df2['Grouped']:\n",
    "    row_list = row.split(',')\n",
    "    count.update(Counter(combinations(row_list, 2)))\n",
    "\n",
    "for key,value in count.most_common(10):\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Exploratory Data Analysis & Visualization\n",
    "# 1. Visualization packages\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 2. catplot\n",
    "sns.catplot(x='percent_change', y='values', hue='name', data=df7, kind='point')\n",
    "\n",
    "# 3. lineplot\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "sns.lineplot(x='timestamp', y='quote.USD.price', data = df10)\n",
    "\n",
    "# 4. boxplot\n",
    "df.boxplot(column=['gross'])\n",
    "\n",
    "# 5. regression plot\n",
    "sns.regplot(x=\"gross\", y=\"budget\", data=df)\n",
    "\n",
    "# 6. Pearson:\n",
    "df.corr(method ='pearson')\n",
    "\n",
    "# 7. Kendall:\n",
    "df.corr(method ='kendall')\n",
    "\n",
    "# 8. spearman:\n",
    "df.corr(method ='spearman')\n",
    "\n",
    "# 9. Using factorize - this assigns a random numeric value for each unique categorical value\n",
    "df.apply(lambda x: x.factorize()[0]).corr(method='pearson')\n",
    "\n",
    "# 10. heat map\n",
    "correlation_matrix = df.apply(lambda x: x.factorize()[0]).corr(method='pearson')\n",
    "\n",
    "sns.heatmap(correlation_matrix, annot = True)\n",
    "\n",
    "plt.title(\"Correlation matrix for Movies\")\n",
    "\n",
    "plt.xlabel(\"Movie features\")\n",
    "\n",
    "plt.ylabel(\"Movie features\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 11. Scatter\n",
    "plt.scatter(x=df['budget'], y=df['gross'], alpha=0.5)\n",
    "plt.title('Budget vs Gross Earnings')\n",
    "plt.xlabel('Gross Earnings')\n",
    "plt.ylabel('Budget for Film')\n",
    "plt.show()\n",
    "\n",
    "#12 swarmplot\n",
    "sns.swarmplot(x=\"rating\", y=\"gross\", data=df)\n",
    "\n",
    "#13 stripplot\n",
    "sns.stripplot(x=\"rating\", y=\"gross\", data=df)\n",
    "\n",
    "# 14. bar chart\n",
    "plt.bar(keys,all_data.groupby(['City']).sum()['Sales'])\n",
    "plt.ylabel('Sales in USD ($)')\n",
    "plt.xlabel('Month number')\n",
    "plt.xticks(keys, rotation='vertical', size=8)\n",
    "plt.show()\n",
    "\n",
    "# 15. plot\n",
    "keys = [pair for pair, df in all_data.groupby(['Hour'])]\n",
    "\n",
    "plt.plot(keys, all_data.groupby(['Hour']).count()['Count'])\n",
    "plt.xticks(keys)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# 16. adding-a-y-axis-label-to-secondary-y-axis-in-matplotlib\n",
    "\n",
    "prices = all_data.groupby('Product').mean()['Price Each']\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax1.bar(keys, quantity_ordered, color='g')\n",
    "ax2.plot(keys, prices, color='b')\n",
    "\n",
    "ax1.set_xlabel('Product Name')\n",
    "ax1.set_ylabel('Quantity Ordered', color='g')\n",
    "ax2.set_ylabel('Price ($)', color='b')\n",
    "ax1.set_xticklabels(keys, rotation='vertical', size=8)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# 17 %matplotlib inline = plt.show()\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Useful function and statements\n",
    "# 1. FOR \n",
    "# \n",
    "entity_input = str(8211)\n",
    "counter = 0\n",
    "for i in query_points:\n",
    "    to_save = bing_maps_query(i[0], i[1], 1, entity_input, bing_maps_key, base_url)\n",
    "\n",
    "    with open(pwd + f\"/{str(counter)}.txt\", 'w') as outfile:\n",
    "        json.dump(to_save, outfile) # Save to make sure that I don't lose data\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "\n",
    "the_list = range(3, 10)\n",
    "    \n",
    "    for i in the_list: \n",
    "        i += 2 \n",
    "        print(i)\n",
    "\n",
    "\n",
    "# 2. if\n",
    "\n",
    "df_numerized = df\n",
    "\n",
    "for col_name in df_numerized.columns:\n",
    "    if(df_numerized[col_name].dtype == 'object'):\n",
    "        df_numerized[col_name]= df_numerized[col_name].astype('category')\n",
    "        df_numerized[col_name] = df_numerized[col_name].cat.codes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3. With\n",
    "with open(pwd + \"/7.txt\") as f:\n",
    "    json_data = json.load(f)\n",
    "    data = json_data.get(\"d\").get(\"results\")   \n",
    "\n",
    "\n",
    "    \n",
    "# 4. while\n",
    " \n",
    "x = 0\n",
    "    \n",
    "while x < 5:\n",
    "    x +=1 \n",
    "    print(x)\n",
    "\n",
    "# 5.try except\n",
    "try:\n",
    "    2 / 0 # You can't divide by 0 and this will throw an exception\n",
    "except ZeroDivisionError:\n",
    "    print(\"Can't divide by 0\")\n",
    "\n",
    "    \n",
    "- Try - Except: Try will help you block errors from your code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimal_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38f4aa199269a897896c02a17c020ec08ff6134a87e9dc2d1fb902ccbbfdc9b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
